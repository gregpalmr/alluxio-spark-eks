#
# The Alluxio Open Foundation licenses this work under the Apache License, version 2.0
# (the "License"). You may not use this work except in compliance with the License, which is
# available at www.apache.org/licenses/LICENSE-2.0
#
# This software is distributed on an "AS IS" basis, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied, as more fully set forth in the License.
#
# See the NOTICE file distributed with this work for information regarding copyright ownership.
#

# This should not be modified in the usual case.
fullnameOverride: alluxio

# Docker Image
image: alluxio/alluxio
imageTag: 2.9.3
imagePullPolicy: IfNotPresent

# If deploying Alluxio Enterprise Edition License,
# replace this with the base64 encoded license generated by
#   cat /path/to/license.json | base64 |  tr -d "\n"
#license: PUT_YOUR_LICENSE_BASE64_VALUE_HERE

# Security Context
user: 1000
group: 1000
fsGroup: 1000

# Service Account
#   If not specified, Kubernetes will assign the 'default'
#   ServiceAccount used for the namespace
serviceAccount:

# Image Pull Secret
#   The secrets will need to be created externally from
#   this Helm chart, but you can configure the Alluxio
#   Pods to use the following list of secrets
# eg:
# imagePullSecrets:
#   - ecr
#   - dev
imagePullSecrets:

# Alluxio Common Site Properties
properties:

  # Alluxio master journal properties
  alluxio.master.journal.type: EMBEDDED
  alluxio.master.journal.folder: /journal
  alluxio.master.embedded.journal.addresses: "alluxio-master-0:19200,alluxio-master-1:19200,alluxio-master-2:19200"

  # Alluxio master metastore properties
  alluxio.master.metastore: ROCKS
  alluxio.master.metastore.dir: /metastore
  alluxio.master.metadata.sync.concurrency.level: 20
  alluxio.master.metadata.sync.executor.pool.size: 10
  alluxio.master.metadata.sync.ufs.prefetch.pool.size: 10

  # Alluxio master root under file system (UFS) properties - other UFSs can be mounted later
  alluxio.master.mount.table.root.ufs: "s3a://PUT_YOUR_S3_BUCKET_NAME_HERE/alluxio_root_ufs/"
  # If Instance IAM roles are used, you do not need to specify
  # access key ID and secret key. If not, then specify them here:
  #alluxio.master.mount.table.root.option.s3a.accessKeyId: "PUT_YOUR_AWS_ACCESS_KEY_ID_HERE"
  #alluxio.master.mount.table.root.option.s3a.secretKey: "PUT_YOUR_AWS_SECRET_KEY_HERE"

  # Alluxio master other properties
  alluxio.master.worker.threads.max: 10000
  alluxio.master.persistence.blacklist: ".spark-staging,.hive-staging,_temporary,_SUCCESS,_delta_log,.tmp"
  alluxio.master.rpc.executor.max.pool.size: 10000
  alluxio.master.audit.logging.enabled: true
  alluxio.master.jvm.monitor.enabled: true

  # Alluxio job_master properties
  alluxio.job.master.worker.timeout: "300sec"
  alluxio.job.master.job.capacity: 200000

  # Alluxio job_worker properties
  alluxio.job.worker.threadpool.size: 20

  # Alluxio user properties
  alluxio.user.file.writetype.default: "CACHE_THROUGH"
  alluxio.user.file.readtype.default: "CACHE"
  alluxio.user.file.replication.durable: 1
  alluxio.user.file.persistence.initial.wait.time: -1
  alluxio.user.file.persist.on.rename: true
  alluxio.user.file.replication.durable: 1
  alluxio.user.file.passive.cache.enabled: false
  alluxio.user.file.persistence.initial.wait.time: -1
  alluxio.user.file.persist.on.rename: true
  alluxio.user.file.master.client.threads: 10000
  alluxio.user.file.buffer.bytes: 64MB
  alluxio.user.block.master.client.threads: 10000
  alluxio.user.block.size.bytes.default: 16MB
  alluxio.user.metrics.collection.enabled: true
  # following properties should be increased if frequent timeouts are observed
  alluxio.user.rpc.retry.max.duration: "2min"
  alluxio.user.rpc.retry.base.sleep: "50ms"
  alluxio.user.network.streaming.keepalive.timeout: "30sec"
  alluxio.user.network.rpc.max.inbound.message.size: "2047MB"
  alluxio.user.streaming.writer.close.timeout: "30min"

  # Alluxio worker properties
  alluxio.worker.network.async.cache.manager.threads.max: 128
  alluxio.worker.network.async.cache.manager.queue.max: 30000
  alluxio.worker.network.block.writer.threads.max: 1024
  alluxio.worker.network.block.reader.threads.max: 3096
  alluxio.worker.network.async.cache.manager.queue.max: 25000
  alluxio.worker.network.keepalive.time: "30sec"
  alluxio.worker.network.keepalive.timeout: "30sec"
  alluxio.worker.network.netty.boss.threads: 1
  alluxio.worker.network.netty.worker.threads: 192
  alluxio.worker.jvm.monitor.enabled: true

# Alluxio worker node cache storage configuration
# - For PROD, recommend using local NVMe or SSD for cache storage, never network storage
#tieredstore:
#  levels:
#  - level: 0
#    alias: SSD
#    mediumtype: SSD,SSD
#    path: /mnt/fast-disks/disk0,/mnt/fast-disks/disk1
#    type: persistentVolumeClaim
#    name: alluxio-cache-worker-nvme0,alluxio-cache-worker-nvme1
#    quota: 270GB,270GB
#    high: 0.95
#    low: 0.7
tieredstore:
  levels:
  - level: 0
    alias: SSD
    mediumtype: SSD,SSD
    path: /mnt/fast-disks/disk0,/mnt/fast-disks/disk1
    type: hostPath
    quota: 270GB,270GB

master:
  count: 3  # 1, or for multi-master mode set this to 3, 5 or 7
  properties:
  nodeSelector:
  tolerations:
  #  - key: "PUT_YOUR_TOLERATION_KEY_HERE"
  #    operator: "Equal"
  #    value: "PUT_YOUR_TOLERATION_VALUE_HERE"
  #    effect: "NoSchedule"
  resources:
    limits:
      cpu: 4
      memory: 18Gi
    requests:
      cpu: 4
      memory: 18Gi
  ports:
    embedded: 19200
    rpc: 19998
    web: 19999
  jvmOptions:
    - "-Xms14g"
    - "-Xmx14g"
    - "-XX:MaxDirectMemorySize=4g"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "19999"
    prometheus.io/path: "/metrics/prometheus/"

jobMaster:
  properties:
  resources:
    limits:
      cpu: 1
      memory: 6Gi
    requests:
      cpu: 1
      memory: 6Gi
  jvmOptions:
    - "-Xms6g"
    - "-Xmx6g"

worker:
  properties:
  tolerations:
  #  - key: "PUT_YOUR_TOLERATION_KEY_HERE"
  #    operator: "Equal"
  #    value: "PUT_YOUR_TOLERATION_VALUE_HERE"
  #    effect: "NoSchedule"
  resources:
    limits:      
      cpu: 8
      memory: 20Gi
    requests:
      cpu: 8
      memory: 20Gi
  ports:
    rpc: 29999
    web: 30000
  jvmOptions:
    - "-Xms16g"
    - "-Xmx16g"
    - "-XX:MaxDirectMemorySize=4g"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "30000"
    prometheus.io/path: "/metrics/prometheus/"

jobWorker:
  properties:
  resources:
    limits:      
      cpu: 1
      memory: 8Gi
    requests:
      cpu: 1
      memory: 8Gi
  jvmOptions:
    - "-Xms8g"
    - "-Xmx8g"

# Deploy the Alluxio proxy for REST API (S3 API too)
proxy:
  enabled: true # Enable this to enable the proxy for REST API (S3 API too)
  env:
  args:
    - proxy
  properties:
  resources:
    requests:
      cpu: 1
      memory: 6Gi
    limits:
      cpu: 1
      memory: 6Gi
  jvmOptions:
    - "-Xms6g"
    - "-Xmx6g"
  ports:
    web: 39999
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to proxy containers
  jvmOptions:
  nodeSelector: {}
  tolerations: []
  podAnnotations: {}
  # The ServiceAccount provided here will have precedence over
  # the global `serviceAccount`
  serviceAccount:

shortCircuit:
  enabled: true
  policy: uuid
  size: 1Mi
  # volumeType controls the type of shortCircuit volume.
  # It can be "persistentVolumeClaim" or "hostPath"
  volumeType: persistentVolumeClaim
  # Attributes to use if the domain socket volume is PVC
  pvcName: alluxio-worker-domain-socket
  accessModes:
    - ReadWriteOnce
  storageClass: standard


journal:
  type: "EMBEDDED"
  ufsType: "local"
  folder: "/journal"
  volumeType: persistentVolumeClaim
  size: 270Gi
  storageClass: "fast-disks"
  accessModes:
    - ReadWriteOnce

metastore:
  volumeType: persistentVolumeClaim # Options: "persistentVolumeClaim" or "emptyDir"
  size: 270Gi
  mountPath: /metastore
  ## Attributes to use when the metastore is persistentVolumeClaim
  storageClass: "fast-disks"
  accessModes:
  - ReadWriteOnce

configmapMounts:

## Secrets
secrets:
  master:
  worker:

##  Metrics System ##

# Settings for Alluxio metrics. Disabled by default.
metrics:
  enabled: true
  # Enable PrometheusMetricsServlet by class name
  PrometheusMetricsServlet:
    enabled: true
  # Pod annotations for Prometheus
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/jobMasterPort: "20002"
    prometheus.io/jobWorkerPort: "30003"
    prometheus.io/path: "/metrics/prometheus/"

# Remote logging server
logserver:
  enabled: false
  replicas: 1
  env:
  # Extra environment variables for the logserver pod
  # Example:
  # JAVA_HOME: /opt/java
  args: # Arguments to Docker entrypoint
    - logserver
  # Properties for the logserver component
  properties:
  resources:
    # The default xmx is 8G
    limits:
      cpu: 1
      memory: 6Gi
    requests:
      cpu: 1
      memory: 1Gi
  ports:
    logging: 45600
  hostPID: false
  hostNetwork: false
  # dnsPolicy will be ClusterFirstWithHostNet if hostNetwork: true
  # and ClusterFirst if hostNetwork: false
  # You can specify dnsPolicy here to override this inference
  # dnsPolicy: ClusterFirst
  # JVM options specific to the logserver container
  jvmOptions:
  nodeSelector:
  tolerations:
  #  - key: "PUT_YOUR_TOLERATION_KEY_HERE"
  #    operator: "Equal"
  #    value: "PUT_YOUR_TOLERATION_VALUE_HERE"
  #    effect: "NoSchedule"

  # volumeType controls the type of log volume.
  # It can be "persistentVolumeClaim" or "hostPath" or "emptyDir"
  volumeType: persistentVolumeClaim
  # Attributes to use if the log volume is PVC
  pvcName: alluxio-logserver
  accessModes:
    - ReadWriteOnce
  # storageClass: "standard"
  # If you are dynamically provisioning PVs, the selector on the PVC should be empty.
  # Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class-1
  # Example:
  selector: {}
  # Attributes to use if the log volume is hostPath
  hostPath: "/tmp/alluxio-logs" # The hostPath directory to use
  # Attributes to use when the log volume is emptyDir
  medium: ""
  size: 100Gi


