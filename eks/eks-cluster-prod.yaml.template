---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: emr-spark-alluxio
  region: us-west-1 # replace with your region
  version: "1.25"
vpc:
  clusterEndpoints:
    publicAccess: true
    privateAccess: true
  publicAccessCIDRs: ["PUT_YOUR_YOUR_PUBLIC_IP_HERE/32"]
availabilityZones: ["us-west-1a","us-west-1b"] # replace with your region
iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
    wellKnownPolicies:
      autoScaler: true
    roleName: eksctl-cluster-autoscaler-role
managedNodeGroups:
  - name: master
    labels:
      fast-disk-node: "pv-nvme"
    instanceType: m5d.4xlarge # 16 vcores, 64 GB MEM, 2 x 300 NVMe SSD, Up to 10GbE Network
    availabilityZones: ["us-west-1a"]
    volumeSize: 100
    volumeType: gp3
    minSize: 3
    maxSize: 3
    desiredCapacity: 3
    tags:
      k8s.io/cluster-autoscaler/enabled: "false"
      k8s.io/cluster-autoscaler/eks-nvme: "owned" 
    ssh:
      allow: true
      publicKeyPath: PUT_YOUR_PATH_TO_PUB_SSH_KEY_HERE # Exampe: ~/.ssh/my_ssh_key.pub
    preBootstrapCommands:
      - |
        # Install NVMe CLI
        yum install nvme-cli -y

        # Get a list of instance-store NVMe drives
        nvme_drives=$(nvme list | grep "Amazon EC2 NVMe Instance Storage" | cut -d " " -f 1 || true)
        readarray -t nvme_drives <<< "$nvme_drives"

        disk_num=0
        for disk in "${nvme_drives[@]}"
        do
          # Format the disk to ext4
          mkfs.ext4 -F $disk

          # Get disk UUID
          uuid=$(blkid -o value -s UUID $disk)

          # Create a filesystem path to mount the disk
          mount_location="/mnt/fast-disks/disk${disk_num}"
          mkdir -p $mount_location

          # Mount the disk
          mount $disk $mount_location

          # Mount the disk during a reboot
          echo $disk $mount_location ext4 defaults,noatime 0 2 >> /etc/fstab
          ((disk_num++))
        done
  - name: worker
    labels:
      fast-disk-node: "pv-nvme"
    instanceType: m5d.8xlarge # 32 vcores, 128 GB MEM, 2 x 600 NVMe SSD, 10GbE Network
    availabilityZones: ["us-west-1a"]
    volumeSize: 100
    volumeType: gp3
    minSize: 3
    maxSize: PUT_YOUR_MAX_WORKER_COUNT_HERE # Example: 10
    desiredCapacity: PUT_YOUR_DESIRED_WORKER_COUNT_HERE # Example: 5
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/eks-nvme: "owned" 
    ssh:
      allow: true
      publicKeyPath: PUT_YOUR_PATH_TO_PUB_SSH_KEY_HERE # Exampe: ~/.ssh/my_ssh_key.pub
    preBootstrapCommands:
      - |
        # Install NVMe CLI
        yum install nvme-cli -y

        # Get a list of instance-store NVMe drives
        nvme_drives=$(nvme list | grep "Amazon EC2 NVMe Instance Storage" | cut -d " " -f 1 || true)
        readarray -t nvme_drives <<< "$nvme_drives"

        disk_num=0
        for disk in "${nvme_drives[@]}"
        do
          # Format the disk to ext4
          mkfs.ext4 -F $disk

          # Get disk UUID
          uuid=$(blkid -o value -s UUID $disk)

          # Create a filesystem path to mount the disk
          mount_location="/mnt/fast-disks/disk${disk_num}"
          mkdir -p $mount_location

          # Mount the disk
          mount $disk $mount_location

          # Mount the disk during a reboot
          echo $disk $mount_location ext4 defaults,noatime 0 2 >> /etc/fstab
          ((disk_num++))
        done
addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest
cloudWatch:
  clusterLogging:
    enableTypes: ["*"]
